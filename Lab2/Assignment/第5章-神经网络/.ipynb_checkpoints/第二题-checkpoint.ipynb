{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二题：神经网络：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验内容：\n",
    "1. 学会梯度下降的基本思想\n",
    "2. 学会使用梯度下降求解线性回归\n",
    "3. 了解归一化处理的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "<img src=\"https://davidham3.github.io/blog/2018/09/11/logistic-regression/Fig0.png\" width=300>\n",
    "\n",
    "我们来完成最简单的线性回归，上图是一个最简单的神经网络，一个输入层，一个输出层，没有激活函数。  \n",
    "我们记输入为$X \\in \\mathbb{R}^{n \\times m}$，输出为$Z \\in \\mathbb{R}^{n}$。输入包含了$n$个样本，$m$个特征，输出是对这$n$个样本的预测值。  \n",
    "输入层到输出层的权重和偏置，我们记为$W \\in \\mathbb{R}^{m}$和$b \\in \\mathbb{R}$。  \n",
    "输出层没有激活函数，所以上面的神经网络的前向传播过程写为：\n",
    "\n",
    "$$\n",
    "Z = XW + b\n",
    "$$\n",
    "\n",
    "我们使用均方误差作为模型的损失函数\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, \\hat{y}) = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "我们通过调整参数$W$和$b$来降低均方误差，或者说是以降低均方误差为目标，学习参数$W$和参数$b$。当均方误差下降的时候，我们认为当前的模型的预测值$Z$与真值$y$越来越接近，也就是说模型正在学习如何让自己的预测值变得更准确。\n",
    "\n",
    "在前面的课程中，我们已经学习了这种线性回归模型可以使用最小二乘法求解，最小二乘法在求解数据量较小的问题的时候很有效，但是最小二乘法的时间复杂度很高，一旦数据量变大，效率很低，实际应用中我们会使用梯度下降等基于梯度的优化算法来求解参数$W$和参数$b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "梯度下降是一种常用的优化算法，通俗来说就是计算出参数的梯度（损失函数对参数的偏导数的导数值），然后将参数减去参数的梯度乘以一个很小的数（下面的公式），来改变参数，然后重新计算损失函数，再次计算梯度，再次进行调整，通过一定次数的迭代，参数就会收敛到最优点附近。\n",
    "\n",
    "在我们的这个线性回归问题中，我们的参数是$W$和$b$，使用以下的策略更新参数：\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。\n",
    "\n",
    "接下来我们会求解损失函数对参数的偏导数。\n",
    "\n",
    "损失函数MSE记为：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$\n",
    "\n",
    "其中，$Z \\in \\mathbb{R}^{n}$是我们的预测值，也就是神经网络输出层的输出值。这里我们有$n$个样本，实际上是将$n$个样本的预测值与他们的真值相减，取平方后加和。\n",
    "\n",
    "我们计算损失函数对参数$W$的偏导数，根据链式法则，可以将偏导数拆成两项，分别求解后相乘：\n",
    "\n",
    "**这里我们以矩阵的形式写出推导过程，感兴趣的同学可以尝试使用单个样本进行推到，然后推广到矩阵形式**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\\n",
    "&= - \\frac{2}{n} X^\\mathrm{T} (y - Z)\\\\\n",
    "&= \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "\\end{aligned}$$\n",
    "\n",
    "同理，求解损失函数对参数$b$的偏导数:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\\n",
    "&= - \\frac{2}{n} \\sum^n_{i=1}(y_i - Z_i)\\\\\n",
    "&= \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**因为参数$b$对每个样本的损失值都有贡献，所以我们需要将所有样本的偏导数都加和。**\n",
    "\n",
    "其中，$\\frac{\\partial \\mathrm{loss}}{\\partial W} \\in \\mathbb{R}^{m}$，$\\frac{\\partial \\mathrm{loss}}{\\partial b} \\in \\mathbb{R}$，求解得到的梯度的维度与参数一致。\n",
    "\n",
    "完成上式两个梯度的计算后，就可以使用梯度下降法对参数进行更新了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练神经网络的基本思路：\n",
    "\n",
    "1. 首先对参数进行初始化，对参数进行随机初始化（也就是取随机值）\n",
    "2. 将样本输入神经网络，计算神经网络预测值 $Z$\n",
    "3. 计算损失值MSE\n",
    "4. 通过 $Z$ 和 $y$ ，以及 $X$ ，计算参数的梯度\n",
    "5. 使用梯度下降更新参数\n",
    "6. 循环1-5步，**在反复迭代的过程中可以看到损失值不断减小的现象，如果没有下降说明出了问题**\n",
    "\n",
    "接下来我们来实现这个最简单的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用kaggle房价数据，选3列作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('data/kaggle_house_price_prediction/kaggle_hourse_price_train.csv')\n",
    "\n",
    "# 使用这3列作为特征\n",
    "features = ['LotArea', 'BsmtUnfSF', 'GarageArea']\n",
    "target = 'SalePrice'\n",
    "data = data[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40%做测试集，60%做训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainY, testY = train_test_split(data[features], data[target], test_size = 0.4, random_state = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集876个样本，3个特征，测试集584个样本，3个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((876, 3), (876,), (584, 3), (584,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要初始化参数$W$和$b$，其中$W \\in \\mathbb{R}^m$，$b \\in \\mathbb{R}$，初始化的策略是将$W$初始化成一个随机数矩阵，参数$b$为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(m):\n",
    "    '''\n",
    "    参数初始化，将W初始化成一个随机向量，b是一个长度为1的向量\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m: int, 特征数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    W: np.ndarray, shape = (m, ), 参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, ), 参数b\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 指定随机种子，这样生成的随机数就是固定的了，这样就可以与下面的测试样例进行比对\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    W = np.random.normal(size = (m, )) * 0.01\n",
    "    \n",
    "    b = np.zeros((1, ))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要完成输入矩阵$X$在神经网络中的计算，也就是完成 $Z = XW + b$ 的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, b):\n",
    "    '''\n",
    "    前向传播，计算Z = XW + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 完成Z = XW + b的计算\n",
    "    # YOUR CODE HERE\n",
    "    Z = np.dot(X,W) + np.ones(X.shape[0]) * b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.37377228144393\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = forward(trainX, Wt, bt)\n",
    "print(tmp.mean()) # -28.37377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来编写损失函数，我们以均方误差(MSE)作为损失函数，需要大家实现MSE的计算：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE，均方误差\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, )，预测值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss: float，损失值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 计算MSE\n",
    "    # YOUR CODE HERE\n",
    "    loss = np.sum(np.square(y_true - y_pred))/y_true.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39381033680.460075\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = mse(trainY, forward(trainX, Wt, bt))\n",
    "print(tmp) # 39381033680.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们要完成梯度的计算，也就是计算出损失函数对参数的偏导数的导数值：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} = \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} = \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Z, y_true):\n",
    "    '''\n",
    "    计算梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    n = len(y_true)\n",
    "    \n",
    "    # 计算W的梯度\n",
    "    # YOUR CODE HERE\n",
    "    dW = 2 * np.dot(X.T, (Z - y_true)) / n\n",
    "    # 计算b的梯度\n",
    "    # YOUR CODE HERE\n",
    "    db = 2 * np.sum(Z - y_true) / n\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "-1532030241.2528896\n",
      "-364308.5557637409\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print(dWt.shape) # (3,)\n",
    "print(dWt.mean()) # -1532030241.25\n",
    "print(dbt.mean()) # -364308.555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分需要实现梯度下降的函数\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(dW, db, W, b, learning_rate):\n",
    "    '''\n",
    "    梯度下降，参数更新，不需要返回值，W和b实际上是以引用的形式传入到函数内部，\n",
    "    函数内改变W和b会直接影响到它们本身，所以不需要返回值\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 更新W\n",
    "    W -= learning_rate * dW\n",
    "    \n",
    "    # 更新b\n",
    "    b -= learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "(3,)\n",
      "15320302.416581338\n",
      "3643.0855576374092\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "update(dWt, dbt, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 3643.08555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成整个参数更新的过程，先计算梯度，再更新参数，将compute_gradient和update组装在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, Z, y_true, W, b, learning_rate):\n",
    "    '''\n",
    "    使用compute_gradient和update函数，先计算梯度，再更新参数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 计算参数的梯度\n",
    "    # YOUR CODE HERE\n",
    "    dWt, dbt = compute_gradient(X, Z, y_true)\n",
    "    # 更新参数\n",
    "    # YOUR CODE HERE\n",
    "    update(dWt, dbt, W, b, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "(3,)\n",
      "15320302.416581338\n",
      "3643.0855576374092\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "backward(trainX, Zt, trainY, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 3643.08555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainX, trainY, testX, testY, W, b, epochs, learning_rate = 0.01, verbose = False):\n",
    "    '''\n",
    "    训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播，更新参数\n",
    "    同时记录训练集和测试集上的损失值，后面画图用。然后循环往复，直到达到最大迭代次数epochs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trainX: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    trainY: np.ndarray, shape = (n, ), 训练集标记\n",
    "    \n",
    "    testX: np.ndarray, shape = (n_test, m)，测试集\n",
    "    \n",
    "    testY: np.ndarray, shape = (n_test, )，测试集的标记\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，参数b\n",
    "    \n",
    "    epochs: int, 要迭代的轮数\n",
    "    \n",
    "    learning_rate: float, default 0.01，学习率\n",
    "    \n",
    "    verbose: boolean, default False，是否打印损失值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # 这里我们要将神经网络的输出值保存起来，因为后面反向传播的时候需要这个值\n",
    "        Z = forward(trainX, W, b)\n",
    "        \n",
    "        # 计算训练集的损失值\n",
    "        training_loss = mse(trainY, Z)\n",
    "        \n",
    "        # 计算测试集的损失值        \n",
    "        testing_loss = mse(testY, forward(testX, W, b))\n",
    "        \n",
    "        # 将损失值存起来\n",
    "        training_loss_list.append(training_loss)\n",
    "        testing_loss_list.append(testing_loss)\n",
    "        \n",
    "        # 打印损失值，debug用\n",
    "        if verbose:\n",
    "            print('epoch %s training loss: %s'%(epoch+1, training_loss))\n",
    "            print('epoch %s testing loss: %s'%(epoch+1, testing_loss))\n",
    "            print()\n",
    "        \n",
    "        # 反向传播，参数更新\n",
    "        backward(trainX, Z, trainY, W, b, learning_rate)\n",
    "        \n",
    "    return training_loss_list, testing_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "[39381033680.460075, 3.390230782482131e+23]\n",
      "[38555252685.09387, 4.1516070231815876e+23]\n",
      "-57055790600890.94\n",
      "-8824267814.591057\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean())          # 0.00405243937693\n",
    "print(bt.mean())          # 0.0\n",
    "\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, Wt, bt, 2, learning_rate = 0.01, verbose = False)\n",
    "\n",
    "print(training_loss_list) # [39381033680.460075, 3.3902307664083424e+23]\n",
    "print(testing_loss_list)  # [38555252685.093872, 4.1516070070405267e+23]\n",
    "print(Wt.mean())          # -5.70557904608e+13\n",
    "print(bt.mean())          # -8824267814.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 检查\n",
    "\n",
    "编写一个绘制损失值变化曲线的函数\n",
    "\n",
    "一般我们通过绘制损失函数的变化曲线来判断模型的拟合状态。\n",
    "\n",
    "一般来说，随着迭代轮数的增加，训练集的loss在下降，而测试集的loss在上升，这说明我们正在不断地让模型在训练集上表现得越来越好，在测试集上表现得越来越糟糕，这就是过拟合的体现。  \n",
    "\n",
    "如果训练集loss和测试集loss共同下降，这就是我们想要的结果，说明模型正在很好的学习。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(training_loss_list, testing_loss_list):\n",
    "    '''\n",
    "    绘制损失值变化曲线\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(training_loss_list, label = 'training loss')\n",
    "    plt.plot(testing_loss_list, label = 'testing loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这些函数就是完成整个神经网络需要的函数了\n",
    "\n",
    "|函数名|功能|\n",
    "|-|-|\n",
    "|initialize | 参数初始化|\n",
    "|forward | 给定数据，计算神经网络的输出值|\n",
    "|mse | 给定真值，计算神经网络的预测值与真值之间的差距|\n",
    "|backward | 计算参数的梯度，并实现参数的更新|\n",
    "|compute_gradient | 计算参数的梯度|\n",
    "|update | 参数的更新|\n",
    "|backward | 计算参数梯度，并且更新参数|\n",
    "|train | 训练神经网络|\n",
    "|plot_loss_curve | 绘制损失函数的变化曲线|\n",
    "\n",
    "我们使用参数初始化函数和训练函数，完成神经网络的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss: 39381033680.460075\n",
      "epoch 1 testing loss: 38555252685.09387\n",
      "\n",
      "epoch 2 training loss: 3.390230782482131e+23\n",
      "epoch 2 testing loss: 4.1516070231815876e+23\n",
      "\n",
      "epoch 3 training loss: 5.055503104347332e+36\n",
      "epoch 3 testing loss: 6.193449607112209e+36\n",
      "\n",
      "epoch 4 training loss: 7.538764120634843e+49\n",
      "epoch 4 testing loss: 9.23567624315129e+49\n",
      "\n",
      "epoch 5 training loss: 1.1241801912459151e+63\n",
      "epoch 5 testing loss: 1.3772236581164421e+63\n",
      "\n",
      "epoch 6 training loss: 1.6763770323182338e+76\n",
      "epoch 6 testing loss: 2.053715344583035e+76\n",
      "\n",
      "epoch 7 training loss: 2.4998127314176736e+89\n",
      "epoch 7 testing loss: 3.0624994652885115e+89\n",
      "\n",
      "epoch 8 training loss: 3.7277197024801525e+102\n",
      "epoch 8 testing loss: 4.566797925345666e+102\n",
      "\n",
      "epoch 9 training loss: 5.5587740655989845e+115\n",
      "epoch 9 testing loss: 6.810007161577326e+115\n",
      "\n",
      "epoch 10 training loss: 8.289241568194436e+128\n",
      "epoch 10 testing loss: 1.0155079839059058e+129\n",
      "\n",
      "epoch 11 training loss: 1.23609135692545e+142\n",
      "epoch 11 testing loss: 1.5143250820572956e+142\n",
      "\n",
      "epoch 12 training loss: 1.8432589159041878e+155\n",
      "epoch 12 testing loss: 2.258160930776412e+155\n",
      "\n",
      "epoch 13 training loss: 2.7486669266187554e+168\n",
      "epoch 13 testing loss: 3.3673686381508774e+168\n",
      "\n",
      "epoch 14 training loss: 4.0988109745730954e+181\n",
      "epoch 14 testing loss: 5.021418708764659e+181\n",
      "\n",
      "epoch 15 training loss: 6.112145215771015e+194\n",
      "epoch 15 testing loss: 7.487937484200676e+194\n",
      "\n",
      "epoch 16 training loss: 9.114428396533584e+207\n",
      "epoch 16 testing loss: 1.1166009253407063e+208\n",
      "\n",
      "epoch 17 training loss: 1.3591431823508873e+221\n",
      "epoch 17 testing loss: 1.6650748341615118e+221\n",
      "\n",
      "epoch 18 training loss: 2.0267537466567347e+234\n",
      "epoch 18 testing loss: 2.4829588982402346e+234\n",
      "\n",
      "epoch 19 training loss: 3.022294341705809e+247\n",
      "epoch 19 testing loss: 3.702587273475272e+247\n",
      "\n",
      "epoch 20 training loss: 4.506844061827702e+260\n",
      "epoch 20 testing loss: 5.521296597949017e+260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 特征数m\n",
    "m = trainX.shape[1]\n",
    "\n",
    "# 参数初始化\n",
    "W, b = initialize(m)\n",
    "\n",
    "# 训练20轮，学习率为0.01\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W, b, 20, learning_rate = 0.01, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制损失值的变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印损失的信息我们可以看到损失值持续上升，这就说明哪里出了问题。但是如果所有的测试样例都通过了，就说明我们的实现是没有问题的。运行下面的测试样例，观察哪里出了问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, W: [-0.00348894  0.00983703  0.00580923]\n",
      "epoch 0, b: [0.]\n",
      "\n",
      "dWt: [-4.18172940e+09 -2.19880296e+08 -1.94481031e+08]\n",
      "db: -364308.5557637409\n",
      "\n",
      "epoch 1, W: [41817293.96016914  2198802.97412493  1944810.31544994]\n",
      "epoch 1, b: [3643.08555764]\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print('epoch 0, W:', Wt)  # [-0.00348894  0.00983703  0.00580923]\n",
    "print('epoch 0, b:', bt)  # [ 0.]\n",
    "print()\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print('dWt:', dWt) # [ -4.18172940e+09  -2.19880296e+08  -1.94481031e+08]\n",
    "print('db:', dbt) # -364308.555764\n",
    "print()\n",
    "\n",
    "update(dWt, dbt, Wt, bt, 0.01)\n",
    "print('epoch 1, W:', Wt)  # [ 41817293.96016914   2198802.97412493   1944810.31544994]\n",
    "print('epoch 1, b:', bt)  # [ 3643.08555764]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们最开始的参数都是在 $10^{-3}$ 这个数量级上，而第一轮迭代时计算出的梯度的数量级在 $10^8$ 左右，这就导致使用梯度下降更新的时候，让参数变成了 $10^6$ 这个数量级左右（学习率为0.01）。产生这样的问题的主要原因是：我们的原始数据 $X$ 没有经过适当的处理，直接扔到了神经网络中进行训练，导致在计算梯度时，由于 $X$ 的数量级过大，导致梯度的数量级变大，在参数更新时使得参数的数量级不断上升，导致参数无法收敛。\n",
    "\n",
    "解决的方法也很简单，对参数进行归一化处理，将其标准化，使均值为0，缩放到 $[-1, 1]$附近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化处理和第一题一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stand = StandardScaler()\n",
    "trainX_normalized = stand.fit_transform(trainX)\n",
    "testX_normalized = stand.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新训练模型，这次我们迭代40轮，学习率设置为0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainX.shape[1]\n",
    "W, b = initialize(m)\n",
    "training_loss_list, testing_loss_list = train(trainX_normalized, trainY, testX_normalized, testY, W, b, 40, learning_rate = 0.1, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算测试集上的MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60305.852679101554"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = forward(testX_normalized, W, b)\n",
    "mse(testY, prediction) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
